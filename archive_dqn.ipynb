{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from game import Game\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mobashota6250\u001b[0m (\u001b[33mobashota6250-tokyo-university-of-science\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mil/oba/poker_nn/poker_env/wandb/run-20250103_231600-5q5y2h5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h' target=\"_blank\">0103</a></strong> to <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany' target=\"_blank\">https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h' target=\"_blank\">https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f34bbd2ecf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# プロジェクトの初期化\n",
    "wandb.init(\n",
    "    project=\"dqn_qmany\",  # プロジェクト名\n",
    "    name=\"0103\",   # 実験名（オプション）\n",
    "    config={               # ハイパーパラメータなど（オプション）\n",
    "        \"gamma\": 0.98,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"batch_size\": 32,\n",
    "        \"epsilon\": 0.10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "# 現在の日付と時刻を取得\n",
    "now = datetime.now()\n",
    "now = str(now.year) + str(now.month) + str(now.day) + str(now.hour) + str(now.minute) + str(now.second)\n",
    "\n",
    "save_dir = \"./models/dqn_qmany/\" + now + \"/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,batch_size):\n",
    "        self.buffer = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        data = (state,action,reward,next_state,done)\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def get_batch(self):\n",
    "        data = random.sample(self.buffer,self.batch_size)\n",
    "        state = np.stack([x[0] for x in data])  # リスト内包表記で明示的にリストを作成\n",
    "        action = np.stack([x[1] for x in data])\n",
    "        reward = np.stack([x[2] for x in data])\n",
    "        next_state = np.stack([x[3] for x in data])\n",
    "        done = np.stack([x[4] for x in data])\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)  # アクションは整数型\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        return state,action,reward,next_state,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, action_size,state_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_size,128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.l3 = nn.Linear(128,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.00005\n",
    "        self.epsilon = 0.1\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 3\n",
    "        \n",
    "        # Define phases and corresponding state sizes\n",
    "        self.phases = [\"preflop\", \"flop\", \"turn\", \"river\",\"show down\"]\n",
    "        self.state_sizes = {\n",
    "            \"preflop\": 12,\n",
    "            \"flop\": 12,\n",
    "            \"turn\": 12,\n",
    "            \"river\": 12,\n",
    "            \"show down\": 12\n",
    "        }\n",
    "\n",
    "        self.replay_buffers = {}\n",
    "        self.qnets = {}\n",
    "        self.qnet_targets = {}\n",
    "        self.optimizers = {}\n",
    "        self.loss_lists = {}\n",
    "\n",
    "        for phase in self.phases:\n",
    "            self.replay_buffers[phase] = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "            self.qnets[phase] = QNet(self.action_size, self.state_sizes[phase])\n",
    "            self.qnet_targets[phase] = QNet(self.action_size, self.state_sizes[phase])\n",
    "            self.optimizers[phase] = optim.Adam(self.qnets[phase].parameters(), lr=self.lr)\n",
    "            self.loss_lists[phase] = []\n",
    "\n",
    "    def sync_qnet(self):\n",
    "        for phase in self.phases:\n",
    "            self.qnet_targets[phase].load_state_dict(self.qnets[phase].state_dict())\n",
    "\n",
    "    def get_action(self, state, mask, phase_index):\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            valid_actions = torch.nonzero(mask).squeeze(-1).numpy()\n",
    "            return np.random.choice(valid_actions)\n",
    "\n",
    "        qs = self.qnets[self.phases[phase_index]](state)\n",
    "        qs_mask = qs * mask\n",
    "        return qs_mask.argmax().item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, current_phase_index, next_phase_index):\n",
    "        current_phase = self.phases[current_phase_index]\n",
    "        next_phase = self.phases[next_phase_index]\n",
    "\n",
    "        replay_buffer = self.replay_buffers[current_phase]\n",
    "        qnet = self.qnets[current_phase]\n",
    "        qnet_target = self.qnet_targets[next_phase]\n",
    "        optimizer = self.optimizers[current_phase]\n",
    "        loss_list = self.loss_lists[current_phase]\n",
    "\n",
    "        next_state = self.preprocess_next_state(next_state, self.state_sizes[next_phase])\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        if len(replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = replay_buffer.get_batch()\n",
    "\n",
    "        qs = qnet(state)\n",
    "        q = qs[np.arange(len(action)), action]\n",
    "\n",
    "        next_qs = qnet_target(next_state)\n",
    "        next_q = next_qs.max(1)[0].detach()\n",
    "        target = reward + (1 - done) * self.gamma * next_q\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q, target)\n",
    "        wandb.log({current_phase: loss})\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_next_state(next_state, target_shape):\n",
    "        if next_state is None:\n",
    "            return np.zeros(target_shape)\n",
    "        return next_state\n",
    "\n",
    "    def copy_from(self, other_agent):\n",
    "        for phase in self.phases:\n",
    "            self.qnets[phase].load_state_dict(other_agent.qnets[phase].state_dict())\n",
    "            self.optimizers[phase].load_state_dict(other_agent.optimizers[phase].state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_from_a(game,agent_a,agent_b,state_a):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        end_flag:roundが終了したかどうかを示す\n",
    "        next_state:roundが終了しているとNoneになる\n",
    "        reward_a:agent_aのreward\n",
    "    \"\"\"\n",
    "    \n",
    "    mask_a = game.one_round.mask(game.one_round.current_index)\n",
    "    action_a_index= agent_a.get_action(state_a,mask_a,game.one_round.current_phase)\n",
    "\n",
    "    index_to_action = {0:\"f\",1:\"c\",2:\"r\"}\n",
    "    action_a_alpa = index_to_action[action_a_index]\n",
    "\n",
    "    reward_a, state_b, current_phase,next_phase_sb = game.step(action_a_alpa)\n",
    "\n",
    "    if action_a_alpa == \"f\":\n",
    "        # next_stateがないので、更新はrewardのみでされる（next_phaseとかは適当です）\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "    \n",
    "    if game.one_round.current_phase == 4:\n",
    "        # next_stateがないので、更新はrewardのみでされる（next_phaseとかは適当です）\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "\n",
    "    action_b_alpa = index_to_action[action_b_index]\n",
    "    reward_b, next_state_a, current_phase_sb, next_phase = game.step(action_b_alpa)\n",
    "\n",
    "    if action_b_alpa == \"f\":\n",
    "        # next_stateがないので、更新はrewardのみでされる（next_phaseとかは適当です）\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "    \n",
    "    if game.one_round.current_phase == 4:\n",
    "        # next_stateがないので、更新はrewardのみでされる（next_phaseとかは適当です）\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "\n",
    "    agent_a.update(state_a,action_a_index,reward_a,next_state_a,False,current_phase,next_phase)\n",
    "    state_a = next_state_a\n",
    "\n",
    "    return False,state_a,reward_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_b(game,state_b,agent_b):\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "    \n",
    "    index_to_action = {0:\"f\",1:\"c\",2:\"r\"}\n",
    "    action_b_alpa = index_to_action[action_b_index]\n",
    "    reward_b, state_a = game.step(action_b_alpa)[0],game.step(action_b_alpa)[1]\n",
    "\n",
    "    if action_b_alpa == \"f\":\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    elif game.one_round.current_phase == 4:\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    else:\n",
    "        end_flag = False\n",
    "        return end_flag,state_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [29:02<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "sync_interval = 50\n",
    "agent_a = DQNAgent()\n",
    "agent_b = DQNAgent()\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "# for episode in range(episodes):\n",
    "    # print(\"epoch:\",episode,\"が始まりました👏\")\n",
    "    game = Game(2,100000,100,5)\n",
    "    # ゲームの初期条件を入手\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # プレイヤーb（index=0）からstartの場合\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(game,state_b,agent_b)\n",
    "\n",
    "        # プレイヤーa（index=1）からstartの場合\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent_a.sync_qnet()\n",
    "        agent_b.copy_from(agent_a)\n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "        flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "        turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "        river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "        torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "        torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "        torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "        torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "# for episode in range(episodes):\n",
    "    # print(\"epoch:\",episode,\"が始まりました👏\")\n",
    "    game = Game(2,100000,100,5)\n",
    "    # ゲームの初期条件を入手\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # プレイヤーb（index=0）からstartの場合\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(game,state_b,agent_b)\n",
    "\n",
    "        # プレイヤーa（index=1）からstartの場合\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent_a.sync_qnet()\n",
    "        agent_b.copy_from(agent_a)\n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "        flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "        turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "        river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "        torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "        torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "        torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "        torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 必要に応じて detach して numpy 配列に変換\n",
    "# preflop_losses = [loss.detach().numpy() for loss in agent_a.loss_lists[\"preflop\"]]\n",
    "\n",
    "# # グラフを描画\n",
    "# plt.figure(figsize=(14, 6))\n",
    "# plt.plot(preflop_losses, marker='o', linestyle='-', color='b', label='Preflop Losses')\n",
    "\n",
    "# # グラフのタイトルやラベルを設定\n",
    "# plt.title('Preflop Loss Over Time', fontsize=16)\n",
    "# plt.xlabel('Update Steps', fontsize=12)\n",
    "# plt.ylabel('Loss Value', fontsize=12)\n",
    "\n",
    "# # グリッドと凡例を追加\n",
    "# plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# plt.legend(fontsize=12)\n",
    "\n",
    "# # グラフを表示\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3925127/2087294355.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_a.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/dqn_many/20251323160/preflop.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m phases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreflop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mriver\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase \u001b[38;5;129;01min\u001b[39;00m phases:\n\u001b[0;32m----> 8\u001b[0m     agent_a\u001b[38;5;241m.\u001b[39mqnets[phase]\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dqn_many/20251323160/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     agent_b\u001b[38;5;241m.\u001b[39mqnets[phase]\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/dqn_many/20251323160/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m phase \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/dqn_many/20251323160/preflop.pth'"
     ]
    }
   ],
   "source": [
    "### 保存していたデータを復刻\n",
    "\n",
    "save_agent_a = DQNAgent()\n",
    "save_agent_b = DQNAgent()\n",
    "\n",
    "phases = [\"preflop\", \"flop\", \"turn\", \"river\"]\n",
    "\n",
    "# for phase in phases:\n",
    "#     /home/mil/oba/poker_nn/poker_env/models/dqn_qmany/20251323160/flop.pth\n",
    "#     save_agent_a.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))\n",
    "\n",
    "#     save_agent_b.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(100):\n",
    "    # print(\"episode:\",episode,\"が始まりました👏\")\n",
    "    # ゲームの設定\n",
    "    # プレイヤーは二人\n",
    "    game = Game(2,100000,100,3,True)\n",
    "    # ゲームの初期条件を入手\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # 各ラウンド\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(state_b,agent_b,game)\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        agent_b.copy_from(agent_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
