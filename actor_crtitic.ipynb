{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Actor-criticã«ã‚ˆã‚‹ãƒãƒ¼ã‚«ãƒ¼ã®å¼·åŒ–å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from env.game import Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lossã®è¨˜éŒ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–\n",
    "wandb.init(\n",
    "    project=\"actor_critic\",  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå\n",
    "    name=\"æ­£è¦åŒ–\",   # å®Ÿé¨“åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    config={               # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        \"pi_learning_rate\": 0.0002,\n",
    "        \"v_learning_rate\": 0.0005,\n",
    "        \"action_size\":6,\n",
    "        \"gammma\":0.1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å„é–¢æ•°ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### çŠ¶æ…‹ä¾¡å€¤é–¢æ•°(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self,state_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_size, 100)\n",
    "        self.l2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ–¹ç­–(Ï€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, action_size,state_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_size, 100)\n",
    "        self.l2 = nn.Linear(100, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.softmax(self.l2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.8\n",
    "        self.lr_pi = 0.0002\n",
    "        self.lr_v = 0.0005\n",
    "        self.action_size = 6\n",
    "        # actionã¯6ã¤\n",
    "        # fold, check, call, raise_2, raise_3, raise_5  \n",
    "        self.phases = [\"preflop\", \"flop\", \"turn\", \"river\",\"show down\"]\n",
    "        self.state_sizes = {\n",
    "            \"preflop\": 17,\n",
    "            \"flop\": 17,\n",
    "            \"turn\": 17,\n",
    "            \"river\": 17,\n",
    "            \"show down\": 17\n",
    "        }\n",
    "        self.pis = {}\n",
    "        self.vs = {}\n",
    "        self.optimizer_pis = {}\n",
    "        self.optimizer_vs = {}\n",
    "        self.loss_v_lists = {}\n",
    "        self.loss_pi_lists = {}\n",
    "        for phase in self.phases:\n",
    "            self.pis[phase] = PolicyNet(self.action_size ,self.state_sizes[phase])\n",
    "            self.vs[phase] = ValueNet(self.state_sizes[phase])\n",
    "            self.optimizer_pis[phase] = optim.Adam(self.pis[phase].parameters(), lr=self.lr_pi)\n",
    "            self.optimizer_vs[phase] = optim.Adam(self.vs[phase].parameters(), lr=self.lr_v)\n",
    "            self.loss_v_lists[phase] = []\n",
    "            self.loss_pi_lists[phase] = []\n",
    "\n",
    "    def get_action(self, state,mask,phase_index):\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float32)\n",
    "        mask = torch.tensor(mask,dtype=torch.float32)\n",
    "\n",
    "        probs = self.pis[self.phases[phase_index]](state)\n",
    "        probs = probs[0]\n",
    "        probs = probs * mask\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample().item()\n",
    "        return action, probs[action]\n",
    "\n",
    "    def update(self, state, action_prob, reward, next_state, player_done,current_phase_idx,next_phase_idx):\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float32)\n",
    "\n",
    "        current_phase = self.phases[current_phase_idx]\n",
    "        next_phase = self.phases[next_phase_idx]\n",
    "\n",
    "        if player_done:\n",
    "            target = torch.tensor(reward, dtype=torch.float32)\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state[np.newaxis, :], dtype=torch.float32)\n",
    "            target = reward + self.gamma * self.vs[next_phase](next_state)\n",
    "\n",
    "        target = target.detach()\n",
    "        v = self.vs[current_phase](state)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss_v = loss_fn(v, target)\n",
    "        # wandb.log({f\"loss_v:{current_phase}\": loss_v.item()})\n",
    "\n",
    "        delta = target - v\n",
    "        loss_pi = -torch.log(action_prob) * delta.item()\n",
    "        loss_pi = loss_pi.float()\n",
    "        # wandb.log({f\"loss_pi:{current_phase}\": loss_pi.item()})\n",
    "\n",
    "        self.loss_v_lists[current_phase].append(loss_v)\n",
    "        self.loss_pi_lists[current_phase].append(loss_pi)\n",
    "\n",
    "        self.optimizer_vs[current_phase].zero_grad()\n",
    "        self.optimizer_pis[current_phase].zero_grad()\n",
    "        loss_v.backward()\n",
    "        loss_pi.backward()\n",
    "        self.optimizer_vs[current_phase].step()\n",
    "        self.optimizer_pis[current_phase].step()\n",
    "\n",
    "    def copy_from(self, other_agent):\n",
    "        for phase in self.phases:\n",
    "            self.pis[phase].load_state_dict(other_agent.pis[phase].state_dict())\n",
    "            self.vs[phase].load_state_dict(other_agent.vs[phase].state_dict())\n",
    "\n",
    "            self.optimizer_pis[phase].load_state_dict(other_agent.optimizer_pis[phase].state_dict())\n",
    "            self.optimizer_vs[phase].load_state_dict(other_agent.optimizer_vs[phase].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_from_a(game,agent_a,agent_b,state_a,train_mode=True):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        end_flag:roundãŒçµ‚äº†ã—ãŸã‹ã©ã†ã‹ã‚’ç¤ºã™\n",
    "        next_state:roundãŒçµ‚äº†ã—ã¦ã„ã‚‹ã¨Noneã«ãªã‚‹\n",
    "        reward_a:agent_aã®reward\n",
    "    \"\"\"\n",
    "    mask_a = game.one_round.mask(game.one_round.current_index)\n",
    "    player0_win_flag = game.one_round.winner_index_truth == 0\n",
    "    action_a_index, prob_a = agent_a.get_action(state_a,mask_a,game.one_round.current_phase)\n",
    "\n",
    "    index_to_action = {0:\"f\",1:\"check\",2:\"call\",3:\"r_2\",4:\"r_3\",5:\"r_5\"}\n",
    "    action_a = index_to_action[action_a_index]\n",
    "\n",
    "    # agent_b_stack_before = game.players[1].stack\n",
    "    # if game.one_round.current_phase == 0 and game.one_round.players[0].last_player_act == None:\n",
    "    #     agent_b_first_bet = game.players[1].first_bet_amount\n",
    "    # else:\n",
    "    #     agent_b_first_bet = 0\n",
    "\n",
    "    reward_a, state_b,current_phase,next_phase_sb = game.step(action_a)\n",
    "\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼0ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n",
    "    if action_a == \"f\" or game.one_round.current_phase == 4:\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™ï¼‰\n",
    "        \n",
    "        if train_mode:\n",
    "            # print(player0_win_flag,action_a,reward_a)\n",
    "            # reward_a = process_value(reward_a)\n",
    "            agent_a.update(state_a,prob_a,reward_a,None,True,current_phase,current_phase)\n",
    "        end_flag = True\n",
    "        return end_flag,None,reward_a\n",
    "\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index, prob_b = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "\n",
    "    action_b = index_to_action[action_b_index]\n",
    "    reward_b, next_state_a, current_phase_sb, next_phase = game.step(action_b)\n",
    "\n",
    "    # if game.one_round.winner_index_truth == 0:\n",
    "    #     agent_b_stack_after = game.players[1].stack\n",
    "    #     reward_a = (agent_b_stack_before - agent_b_stack_after + agent_b_first_bet)/1000\n",
    "\n",
    "    # if game.one_round.winner_index_truth == 0:\n",
    "    #     reward_a += agent_b_first_bet/1000\n",
    "\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼1ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n",
    "    if action_b == \"f\" or game.one_round.current_phase == 4:\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™\n",
    "        if train_mode:\n",
    "            # print(player0_win_flag,action_a,reward_a)\n",
    "            # reward_a = process_value(reward_a)\n",
    "            agent_a.update(state_a,prob_a,reward_a,None,True,current_phase,current_phase)\n",
    "        end_flag = True\n",
    "        return end_flag,None,reward_a\n",
    "\n",
    "    if train_mode:\n",
    "        # print(player0_win_flag,action_a,reward_a)\n",
    "        # reward_a = process_value(reward_a)\n",
    "        agent_a.update(state_a,prob_a,reward_a,next_state_a,False,current_phase,next_phase)\n",
    "    state_a = next_state_a\n",
    "    end_flag = False\n",
    "\n",
    "    return end_flag,state_a,reward_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_b(state_b,agent_b,game,train_mode=False):\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index, prob_b = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "\n",
    "    index_to_action = {0:\"f\",1:\"check\",2:\"call\",3:\"r_2\",4:\"r_3\",5:\"r_5\"}\n",
    "    action_b = index_to_action[action_b_index]\n",
    "    reward_b, state_a,current_phase,next_phase = game.step(action_b)\n",
    "\n",
    "    if action_b == \"f\":\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    elif game.one_round.current_phase == 4:\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    else:\n",
    "        end_flag = False\n",
    "        return end_flag,state_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å­¦ç¿’start!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player0ãŒagent_a\n",
    "# player1ãŒagent_b\n",
    "\n",
    "agent_a = Agent()\n",
    "agent_b = Agent()\n",
    "episodes = 10000\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    # print(\"episode:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    \n",
    "    # ã‚²ãƒ¼ãƒ ã®è¨­å®š\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯äºŒäºº\n",
    "    game = Game(2,100000,100,6,False)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # å„ãƒ©ã‚¦ãƒ³ãƒ‰\n",
    "        if game.one_round.current_index == 1:\n",
    "            end_flag, state_a = agent_action_b(state_b,agent_b,game)\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        agent_b.copy_from(agent_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’ç¢ºèªã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(episodes):\n",
    "episodes =  100\n",
    "for episode in range(episodes):\n",
    "    # print(\"episode:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    # ã‚²ãƒ¼ãƒ ã®è¨­å®š\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯äºŒäºº\n",
    "    game = Game(2,100000,100,10,True)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # å„ãƒ©ã‚¦ãƒ³ãƒ‰\n",
    "        if game.one_round.current_index == 1:\n",
    "            end_flag, state_a = agent_action_b(state_b,agent_b,game,False)\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹\n",
    "\n",
    "\"train/models/actor_critic/\"ã«ä¿å­˜ã•ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "# ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—\n",
    "now = datetime.now()\n",
    "now = str(now.year) + str(now.month) + str(now.day) + str(now.hour) + str(now.minute) + str(now.second)\n",
    "\n",
    "save_dir_pi = \"./train/models/actor_critic/\"+ now +\"/pi/pytorch/\"\n",
    "save_dir_v= \"./train/models/actor_critic/\" +now +\"/v/pytorch/\"\n",
    "\n",
    "os.makedirs(save_dir_pi, exist_ok=True)\n",
    "os.makedirs(save_dir_v, exist_ok=True)\n",
    "\n",
    "p_preflop_save_path = os.path.join(save_dir_pi, \"preflop.pth\")\n",
    "p_flop_save_path = os.path.join(save_dir_pi, \"flop.pth\")\n",
    "p_turn_save_path = os.path.join(save_dir_pi, \"turn.pth\")\n",
    "p_river_save_path = os.path.join(save_dir_pi, \"river.pth\")\n",
    "torch.save(agent_a.pis[\"preflop\"].state_dict(), p_preflop_save_path)\n",
    "torch.save(agent_a.pis[\"flop\"].state_dict(), p_flop_save_path)\n",
    "torch.save(agent_a.pis[\"turn\"].state_dict(), p_turn_save_path)\n",
    "torch.save(agent_a.pis[\"river\"].state_dict(), p_river_save_path)\n",
    "\n",
    "v_preflop_save_path = os.path.join(save_dir_v, \"preflop.pth\")\n",
    "v_flop_save_path = os.path.join(save_dir_v, \"flop.pth\")\n",
    "v_turn_save_path = os.path.join(save_dir_v, \"turn.pth\")\n",
    "v_river_save_path =os.path.join(save_dir_v, \"river.pth\")\n",
    "\n",
    "torch.save(agent_a.vs[\"preflop\"].state_dict(), v_preflop_save_path)\n",
    "torch.save(agent_a.vs[\"flop\"].state_dict(), v_flop_save_path)\n",
    "torch.save(agent_a.vs[\"turn\"].state_dict(), v_turn_save_path)\n",
    "torch.save(agent_a.vs[\"river\"].state_dict(), v_river_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä¿å­˜ã—ã¦ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã§Agentã‚’è¨­å®šã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ä¿å­˜ã—ã¦ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚’å¾©åˆ»\n",
    "\n",
    "test_agent_a = Agent()\n",
    "test_agent_b = Agent()\n",
    "\n",
    "path = 'train/models/actor_critic/nonnormal/'\n",
    "\n",
    "phases = [\"preflop\", \"flop\", \"turn\", \"river\"]\n",
    "for phase in phases:\n",
    "    test_agent_a.pis[phase].load_state_dict(torch.load(path + 'pi/pytorch/' + phase +'.pth'))\n",
    "    test_agent_a.vs[phase].load_state_dict(torch.load(path + 'v/pytorch/' + phase +'.pth'))\n",
    "    test_agent_b.pis[phase].load_state_dict(torch.load(path + 'pi/pytorch/' + phase +'.pth'))\n",
    "    test_agent_b.vs[phase].load_state_dict(torch.load(path + 'v/pytorch/' + phase +'.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¾©æ´»ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’ç¢ºèªã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(100):\n",
    "    # print(\"episode:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    # ã‚²ãƒ¼ãƒ ã®è¨­å®š\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯äºŒäºº\n",
    "    game = Game(2,100000,100,3,True)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # å„ãƒ©ã‚¦ãƒ³ãƒ‰\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(state_b,test_agent_b,game)\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,test_agent_a,test_agent_b,state_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorchã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’onnxã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "GPUãªã„ç’°å¢ƒã§ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã›ã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnxãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "import os\n",
    "\n",
    "change_agent = Agent()\n",
    "\n",
    "path = \"train/models/actor_critic/2025112202714/\"\n",
    "\n",
    "# ä½œæˆã—ãŸã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "output_pi_dir = path + 'pi/onnx/'\n",
    "output_v_dir = path + 'v/onnx/'\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆï¼‰\n",
    "os.makedirs(output_pi_dir, exist_ok=True)\n",
    "os.makedirs(output_v_dir, exist_ok=True)\n",
    "\n",
    "phases = [\"preflop\", \"flop\", \"turn\", \"river\"]\n",
    "for phase in phases:\n",
    "    change_agent.pis[phase].load_state_dict(torch.load(path+'pi/pytorch/'+ phase +'.pth'))\n",
    "    change_agent.vs[phase].load_state_dict(torch.load(path+'v/pytorch/'+ phase +'.pth'))\n",
    "\n",
    "    change_agent.pis[phase].eval()\n",
    "    change_agent.vs[phase].eval()\n",
    "\n",
    "    dummy_input = torch.randn(1, change_agent.state_sizes[phase])\n",
    "\n",
    "    torch.onnx.export(change_agent.pis[phase], dummy_input, path + 'pi/onnx/'+ phase +'.onnx', export_params=True, opset_version=11)\n",
    "    torch.onnx.export(change_agent.vs[phase], dummy_input, path + 'v/onnx/'+ phase +'.onnx', export_params=True, opset_version=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
