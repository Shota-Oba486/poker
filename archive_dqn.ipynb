{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from game import Game\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mobashota6250\u001b[0m (\u001b[33mobashota6250-tokyo-university-of-science\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mil/oba/poker_nn/poker_env/wandb/run-20250103_231600-5q5y2h5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h' target=\"_blank\">0103</a></strong> to <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany' target=\"_blank\">https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h' target=\"_blank\">https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/obashota6250-tokyo-university-of-science/dqn_qmany/runs/5q5y2h5h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f34bbd2ecf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–\n",
    "wandb.init(\n",
    "    project=\"dqn_qmany\",  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå\n",
    "    name=\"0103\",   # å®Ÿé¨“åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    config={               # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        \"gamma\": 0.98,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"batch_size\": 32,\n",
    "        \"epsilon\": 0.10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "# ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—\n",
    "now = datetime.now()\n",
    "now = str(now.year) + str(now.month) + str(now.day) + str(now.hour) + str(now.minute) + str(now.second)\n",
    "\n",
    "save_dir = \"./models/dqn_qmany/\" + now + \"/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,batch_size):\n",
    "        self.buffer = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        data = (state,action,reward,next_state,done)\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def get_batch(self):\n",
    "        data = random.sample(self.buffer,self.batch_size)\n",
    "        state = np.stack([x[0] for x in data])  # ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã§æ˜ç¤ºçš„ã«ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        action = np.stack([x[1] for x in data])\n",
    "        reward = np.stack([x[2] for x in data])\n",
    "        next_state = np.stack([x[3] for x in data])\n",
    "        done = np.stack([x[4] for x in data])\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)  # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯æ•´æ•°å‹\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        return state,action,reward,next_state,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, action_size,state_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_size,128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.l3 = nn.Linear(128,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.00005\n",
    "        self.epsilon = 0.1\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 3\n",
    "        \n",
    "        # Define phases and corresponding state sizes\n",
    "        self.phases = [\"preflop\", \"flop\", \"turn\", \"river\",\"show down\"]\n",
    "        self.state_sizes = {\n",
    "            \"preflop\": 12,\n",
    "            \"flop\": 12,\n",
    "            \"turn\": 12,\n",
    "            \"river\": 12,\n",
    "            \"show down\": 12\n",
    "        }\n",
    "\n",
    "        self.replay_buffers = {}\n",
    "        self.qnets = {}\n",
    "        self.qnet_targets = {}\n",
    "        self.optimizers = {}\n",
    "        self.loss_lists = {}\n",
    "\n",
    "        for phase in self.phases:\n",
    "            self.replay_buffers[phase] = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "            self.qnets[phase] = QNet(self.action_size, self.state_sizes[phase])\n",
    "            self.qnet_targets[phase] = QNet(self.action_size, self.state_sizes[phase])\n",
    "            self.optimizers[phase] = optim.Adam(self.qnets[phase].parameters(), lr=self.lr)\n",
    "            self.loss_lists[phase] = []\n",
    "\n",
    "    def sync_qnet(self):\n",
    "        for phase in self.phases:\n",
    "            self.qnet_targets[phase].load_state_dict(self.qnets[phase].state_dict())\n",
    "\n",
    "    def get_action(self, state, mask, phase_index):\n",
    "        state = torch.tensor(state[np.newaxis, :], dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            valid_actions = torch.nonzero(mask).squeeze(-1).numpy()\n",
    "            return np.random.choice(valid_actions)\n",
    "\n",
    "        qs = self.qnets[self.phases[phase_index]](state)\n",
    "        qs_mask = qs * mask\n",
    "        return qs_mask.argmax().item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, current_phase_index, next_phase_index):\n",
    "        current_phase = self.phases[current_phase_index]\n",
    "        next_phase = self.phases[next_phase_index]\n",
    "\n",
    "        replay_buffer = self.replay_buffers[current_phase]\n",
    "        qnet = self.qnets[current_phase]\n",
    "        qnet_target = self.qnet_targets[next_phase]\n",
    "        optimizer = self.optimizers[current_phase]\n",
    "        loss_list = self.loss_lists[current_phase]\n",
    "\n",
    "        next_state = self.preprocess_next_state(next_state, self.state_sizes[next_phase])\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.int64)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        if len(replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = replay_buffer.get_batch()\n",
    "\n",
    "        qs = qnet(state)\n",
    "        q = qs[np.arange(len(action)), action]\n",
    "\n",
    "        next_qs = qnet_target(next_state)\n",
    "        next_q = next_qs.max(1)[0].detach()\n",
    "        target = reward + (1 - done) * self.gamma * next_q\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q, target)\n",
    "        wandb.log({current_phase: loss})\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_next_state(next_state, target_shape):\n",
    "        if next_state is None:\n",
    "            return np.zeros(target_shape)\n",
    "        return next_state\n",
    "\n",
    "    def copy_from(self, other_agent):\n",
    "        for phase in self.phases:\n",
    "            self.qnets[phase].load_state_dict(other_agent.qnets[phase].state_dict())\n",
    "            self.optimizers[phase].load_state_dict(other_agent.optimizers[phase].state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_from_a(game,agent_a,agent_b,state_a):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        end_flag:roundãŒçµ‚äº†ã—ãŸã‹ã©ã†ã‹ã‚’ç¤ºã™\n",
    "        next_state:roundãŒçµ‚äº†ã—ã¦ã„ã‚‹ã¨Noneã«ãªã‚‹\n",
    "        reward_a:agent_aã®reward\n",
    "    \"\"\"\n",
    "    \n",
    "    mask_a = game.one_round.mask(game.one_round.current_index)\n",
    "    action_a_index= agent_a.get_action(state_a,mask_a,game.one_round.current_phase)\n",
    "\n",
    "    index_to_action = {0:\"f\",1:\"c\",2:\"r\"}\n",
    "    action_a_alpa = index_to_action[action_a_index]\n",
    "\n",
    "    reward_a, state_b, current_phase,next_phase_sb = game.step(action_a_alpa)\n",
    "\n",
    "    if action_a_alpa == \"f\":\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™ï¼‰\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "    \n",
    "    if game.one_round.current_phase == 4:\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™ï¼‰\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "\n",
    "    action_b_alpa = index_to_action[action_b_index]\n",
    "    reward_b, next_state_a, current_phase_sb, next_phase = game.step(action_b_alpa)\n",
    "\n",
    "    if action_b_alpa == \"f\":\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™ï¼‰\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "    \n",
    "    if game.one_round.current_phase == 4:\n",
    "        # next_stateãŒãªã„ã®ã§ã€æ›´æ–°ã¯rewardã®ã¿ã§ã•ã‚Œã‚‹ï¼ˆnext_phaseã¨ã‹ã¯é©å½“ã§ã™ï¼‰\n",
    "        agent_a.update(state_a,action_a_index,reward_a,None,True,current_phase,current_phase)\n",
    "        return True,None,reward_a\n",
    "\n",
    "    agent_a.update(state_a,action_a_index,reward_a,next_state_a,False,current_phase,next_phase)\n",
    "    state_a = next_state_a\n",
    "\n",
    "    return False,state_a,reward_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_action_b(game,state_b,agent_b):\n",
    "    mask_b = game.one_round.mask(game.one_round.current_index)\n",
    "    action_b_index = agent_b.get_action(state_b,mask_b,game.one_round.current_phase)\n",
    "    \n",
    "    index_to_action = {0:\"f\",1:\"c\",2:\"r\"}\n",
    "    action_b_alpa = index_to_action[action_b_index]\n",
    "    reward_b, state_a = game.step(action_b_alpa)[0],game.step(action_b_alpa)[1]\n",
    "\n",
    "    if action_b_alpa == \"f\":\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    elif game.one_round.current_phase == 4:\n",
    "        end_flag = True\n",
    "        return end_flag,None\n",
    "    else:\n",
    "        end_flag = False\n",
    "        return end_flag,state_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [29:02<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "sync_interval = 50\n",
    "agent_a = DQNAgent()\n",
    "agent_b = DQNAgent()\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "# for episode in range(episodes):\n",
    "    # print(\"epoch:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    game = Game(2,100000,100,5)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼bï¼ˆindex=0ï¼‰ã‹ã‚‰startã®å ´åˆ\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(game,state_b,agent_b)\n",
    "\n",
    "        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼aï¼ˆindex=1ï¼‰ã‹ã‚‰startã®å ´åˆ\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent_a.sync_qnet()\n",
    "        agent_b.copy_from(agent_a)\n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "        flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "        turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "        river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "        torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "        torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "        torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "        torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "# for episode in range(episodes):\n",
    "    # print(\"epoch:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    game = Game(2,100000,100,5)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼bï¼ˆindex=0ï¼‰ã‹ã‚‰startã®å ´åˆ\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(game,state_b,agent_b)\n",
    "\n",
    "        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼aï¼ˆindex=1ï¼‰ã‹ã‚‰startã®å ´åˆ\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % sync_interval == 0:\n",
    "        agent_a.sync_qnet()\n",
    "        agent_b.copy_from(agent_a)\n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "        flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "        turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "        river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "        torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "        torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "        torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "        torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # å¿…è¦ã«å¿œã˜ã¦ detach ã—ã¦ numpy é…åˆ—ã«å¤‰æ›\n",
    "# preflop_losses = [loss.detach().numpy() for loss in agent_a.loss_lists[\"preflop\"]]\n",
    "\n",
    "# # ã‚°ãƒ©ãƒ•ã‚’æç”»\n",
    "# plt.figure(figsize=(14, 6))\n",
    "# plt.plot(preflop_losses, marker='o', linestyle='-', color='b', label='Preflop Losses')\n",
    "\n",
    "# # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„ãƒ©ãƒ™ãƒ«ã‚’è¨­å®š\n",
    "# plt.title('Preflop Loss Over Time', fontsize=16)\n",
    "# plt.xlabel('Update Steps', fontsize=12)\n",
    "# plt.ylabel('Loss Value', fontsize=12)\n",
    "\n",
    "# # ã‚°ãƒªãƒƒãƒ‰ã¨å‡¡ä¾‹ã‚’è¿½åŠ \n",
    "# plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# plt.legend(fontsize=12)\n",
    "\n",
    "# # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preflop_save_path = os.path.join(save_dir, \"preflop.pth\")\n",
    "flop_save_path = os.path.join(save_dir, \"flop.pth\")\n",
    "turn_save_path = os.path.join(save_dir, \"turn.pth\")\n",
    "river_save_path = os.path.join(save_dir, \"river.pth\")\n",
    "\n",
    "torch.save(agent_a.qnets[\"preflop\"].state_dict(), preflop_save_path)\n",
    "torch.save(agent_a.qnets[\"flop\"].state_dict(), flop_save_path)\n",
    "torch.save(agent_a.qnets[\"turn\"].state_dict(), turn_save_path)\n",
    "torch.save(agent_a.qnets[\"river\"].state_dict(), river_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3925127/2087294355.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_a.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/dqn_many/20251323160/preflop.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m phases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreflop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mriver\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase \u001b[38;5;129;01min\u001b[39;00m phases:\n\u001b[0;32m----> 8\u001b[0m     agent_a\u001b[38;5;241m.\u001b[39mqnets[phase]\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dqn_many/20251323160/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     agent_b\u001b[38;5;241m.\u001b[39mqnets[phase]\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/dqn_many/20251323160/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m phase \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.06-1/envs/poker/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/dqn_many/20251323160/preflop.pth'"
     ]
    }
   ],
   "source": [
    "### ä¿å­˜ã—ã¦ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚’å¾©åˆ»\n",
    "\n",
    "save_agent_a = DQNAgent()\n",
    "save_agent_b = DQNAgent()\n",
    "\n",
    "phases = [\"preflop\", \"flop\", \"turn\", \"river\"]\n",
    "\n",
    "# for phase in phases:\n",
    "#     /home/mil/oba/poker_nn/poker_env/models/dqn_qmany/20251323160/flop.pth\n",
    "#     save_agent_a.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))\n",
    "\n",
    "#     save_agent_b.qnets[phase].load_state_dict(torch.load('models/dqn_many/20251323160/'+ phase +'.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(100):\n",
    "    # print(\"episode:\",episode,\"ãŒå§‹ã¾ã‚Šã¾ã—ãŸğŸ‘\")\n",
    "    # ã‚²ãƒ¼ãƒ ã®è¨­å®š\n",
    "    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯äºŒäºº\n",
    "    game = Game(2,100000,100,3,True)\n",
    "    # ã‚²ãƒ¼ãƒ ã®åˆæœŸæ¡ä»¶ã‚’å…¥æ‰‹\n",
    "    game.game_flag = False\n",
    "\n",
    "    while not game.game_flag:\n",
    "        end_flag = False\n",
    "        state_a = game.one_round.player_state(0)\n",
    "        state_b = game.one_round.player_state(1)\n",
    "        # å„ãƒ©ã‚¦ãƒ³ãƒ‰\n",
    "        if game.one_round.current_index == 0:\n",
    "            end_flag, state_a = agent_action_b(state_b,agent_b,game)\n",
    "        while not end_flag:\n",
    "            end_flag, state_a,reward_a= agent_action_from_a(game,agent_a,agent_b,state_a)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        agent_b.copy_from(agent_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
